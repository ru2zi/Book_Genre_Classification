{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM2ivGihK9qTexHdlaxJjb+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ru2zi/Book_Genre_Classification/blob/main/CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm6iCKq_qB9S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pylab import rcParams\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "import tensorflow as tf\n",
        "from keras import optimizers, Sequential\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
        "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
        "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
        "\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from numpy.random import seed\n",
        "#seed(7)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#SEED = 2023\n",
        "\n",
        "from fastdtw import fastdtw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프 배경을 하얀색으로 설정\n",
        "plt.style.use('default')\n",
        "plt.rcParams['axes.facecolor'] = 'white'"
      ],
      "metadata": {
        "id": "5RmmHWF_qFIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"tf.test.is_built_with_cuda():\", tf.test.is_built_with_cuda())\n",
        "\n",
        "print(\"tf.test.is_gpu_available():\", tf.test.is_gpu_available())\n",
        "\n",
        "# Undocumented feature\n",
        "\n",
        "from tensorflow.python.client import device_lib as _device_lib\n",
        "\n",
        "print(_device_lib.list_local_devices())"
      ],
      "metadata": {
        "id": "fuNSR1dYqGPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_04_01_04_08 = pd.read_excel(\"C:/Users/Admin/Downloads\\Etri/삼천산업 데이터 모음/1차 데이터/1C진동융착기_설비전력_DATA_현황_20230503114027.xlsx\")\n",
        "avg_04_10_04_15 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/1차 데이터/2C진동융착기_설비전력_DATA_현황_20230503114438.xlsx\")\n",
        "avg_04_17_04_22 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/1차 데이터/3C진동융착기_설비전력_DATA_현황_20230503115609.xlsx\")\n",
        "avg_04_24_05_02 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/1차 데이터/4C진동융착기_설비전력_DATA_현황_20230503115838.xlsx\")\n",
        "avg_05_17 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/2차 데이터/XD Nozzle 전력분석/23.05.17진동융착기_설비전력_DATA_현황(3상 평균전류).xlsx\")\n",
        "avg_05_18 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/2차 데이터/XD Nozzle 전력분석/23.05.18진동융착기_설비전력_DATA_현황(3상 평균전류).xlsx\")\n",
        "avg_05_19 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/2차 데이터/XD Nozzle 전력분석/23.05.19진동융착기_설비전력_DATA_현황(3상 평균전류).xlsx\")\n",
        "avg_05_25 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/3차 데이터/23.05.25C진동융착기_설비전력_DATA_현황_20230605144944(3상 평균전류).xlsx\")\n",
        "avg_05_26 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/3차 데이터/23.05.26C진동융착기_설비전력_DATA_현황_20230605145427(3상 평균전류).xlsx\")\n",
        "avg_05_27 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/3차 데이터/23.05.27C진동융착기_설비전력_DATA_현황_20230605145944(3상 평균전류).xlsx\")\n",
        "avg_05_29 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/3차 데이터/23.05.29C진동융착기_설비전력_DATA_현황_20230605151922(3상 평균전류).xlsx\")\n",
        "avg_05_30 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/3차 데이터/23.05.30C진동융착기_설비전력_DATA_현황_20230605152304(3상 평균전류).xlsx\")\n",
        "avg_05_31 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/3차 데이터/23.05.31C진동융착기_설비전력_DATA_현황_20230605152756(3상 평균전류).xlsx\")\n",
        "avg_06_01 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/3차 데이터/23.06.01C진동융착기_설비전력_DATA_현황_20230605153212(3상 평균전류).xlsx\")\n",
        "avg_06_02 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/3차 데이터/23.06.02C진동융착기_설비전력_DATA_현황_20230605153656(3상 평균전류).xlsx\")\n",
        "avg_06_03 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/3차 데이터/23.06.03C진동융착기_설비전력_DATA_현황_20230605154131(3상 평균전류).xlsx\")\n",
        "poor_weld = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/1차 데이터/23.05.02_XD Nozzle 융착 불량 정리.xlsx\")\n",
        "time_model_2= pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/2차 데이터\\XD Nozzle 전력분석/XD Nozzle 용착 모델별 생산 시간.xlsx\")\n",
        "time_model_3= pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/3차 데이터/3회차 XD Nozzle 용착 모델별 생산 시간.xlsx\")\n",
        "weld_strength = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/삼천산업 데이터 모음/3차 데이터/3회차 XD Nozzle 용착 시간별 용착강도 측정 Data.xlsx\")\n",
        "\n",
        "avg_08_07 = pd.read_excel(\"C:/Users/Admin/Downloads/Etri/Dishwashers_Anomaly_Detection/3상 평균 전류.xlsx\")"
      ],
      "metadata": {
        "id": "XiiE4Ou5qIqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 함수 선언"
      ],
      "metadata": {
        "id": "wyQs1nxnqSYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Category열 내용 생성\n",
        "def change_category_by_time_range(Current_Strength_data, time_ranges):\n",
        "    Current_Strength_data['Category'] = np.nan\n",
        "\n",
        "    for start_time, end_time, category in time_ranges:\n",
        "        start_time = pd.to_datetime(start_time)\n",
        "        end_time = pd.to_datetime(end_time)\n",
        "        mask = (Current_Strength_data['날짜'] >= start_time) & (Current_Strength_data['날짜'] <= end_time)\n",
        "        Current_Strength_data.loc[mask, 'Category'] = category\n",
        "\n",
        "    return Current_Strength_data\n",
        "\n",
        "# start_time, end_time 사이에 있는 행 추출 후 column_name값 가져옴\n",
        "def extract_data_between_time(data, start_time, end_time, column_name):\n",
        "    start_time = pd.to_datetime(start_time)\n",
        "    end_time = pd.to_datetime(end_time)\n",
        "    between_time_data = data[(data.index < end_time) & (data.index >= start_time)]\n",
        "    between_time_data = between_time_data[[column_name]]\n",
        "    between_time_data = np.array(between_time_data)\n",
        "    return between_time_data\n",
        "\n",
        "# 용창강도열 부분에 값이 존재하는 경우 가져오고 Category값이 US인지 EU인지에 따라 분류\n",
        "def filter_rows_with_values(data_table, category):\n",
        "    columns_to_check = ['용착강도(좌)_1', '용착강도(좌)_2', '용착강도(좌)_3', '용착강도(좌)_4', '용착강도(좌)_5',\n",
        "                        '용착강도(좌)_6', '용착강도(좌)_7', '용착강도(좌)_8', '용착강도(우)_1', '용착강도(우)_2',\n",
        "                        '용착강도(우)_3', '용착강도(우)_4', '용착강도(우)_5', '용착강도(우)_6', '용착강도(우)_7',\n",
        "                        '용착강도(우)_8']\n",
        "\n",
        "    rows_with_values = []\n",
        "\n",
        "    for index, row in data_table.iterrows():\n",
        "        if not row[columns_to_check].isnull().all():\n",
        "            rows_with_values.append(row)\n",
        "\n",
        "    filtered_data = pd.DataFrame(rows_with_values)\n",
        "\n",
        "    filtered_data_us = filtered_data[filtered_data['Category'] == 'US']\n",
        "    filtered_data_eu = filtered_data[filtered_data['Category'] == 'EU']\n",
        "\n",
        "    if category == 'US':\n",
        "        return filtered_data_us\n",
        "    elif category == 'EU':\n",
        "        return filtered_data_eu\n",
        "    else:\n",
        "        return filtered_data\n",
        "\n",
        "# 해당일, 해당 category에 대해 값을 array 형태로 반환: array([4.30586, 4.31395, 4.39656, ..., 4.90187, 4.63368, 4.53176])\n",
        "def filter_data_by_date_and_category(data, date, column_name, category):\n",
        "    date_data = data[data['날짜'].dt.date == pd.to_datetime(date).date()]\n",
        "    filtered_data = date_data[date_data['Category'] == category][column_name]\n",
        "    filtered_data = np.array(filtered_data)\n",
        "    filtered_data = filtered_data.flatten()\n",
        "    return filtered_data\n",
        "\n",
        "# Peak 데이터 다룰 때 상위 ~% 부분 출력\n",
        "def select_data_based_on_threshold(data, all_data, peaks, properties, column_name, percentile, peak_type):\n",
        "    threshold = np.percentile(properties[column_name], percentile)\n",
        "    selected_rows = data[peaks[properties[column_name] >= threshold]]\n",
        "    selected_rows_prop = all_data[all_data['VALUE'].isin(selected_rows)]\n",
        "    selected_rows_prop = selected_rows_prop[['날짜', 'VALUE']]\n",
        "    selected_rows_prop['Peak'] = peak_type\n",
        "    return selected_rows_prop\n",
        "\n",
        "# 뽑아낸 각각의 prop에서 중복되는 행만 추출 후 결합\n",
        "def combine_selected_data(array_data, whole_data, peaks, properties, percent):\n",
        "    prominences = select_data_based_on_threshold(array_data, whole_data, peaks, properties, \"prominences\", percent, \"prominences\")\n",
        "    width_heights = select_data_based_on_threshold(array_data, whole_data, peaks, properties, \"width_heights\", percent, \"width_heights\")\n",
        "    widths = select_data_based_on_threshold(array_data, whole_data, peaks, properties, \"widths\", percent, \"widths\")\n",
        "    heights = select_data_based_on_threshold(array_data, whole_data, peaks, properties, \"peak_heights\", percent, \"heights\")\n",
        "\n",
        "    combined = pd.concat([heights, width_heights, prominences, widths])\n",
        "    return combined\n",
        "\n",
        "\n",
        "# Peak 지점 기준으로 앞,뒤 지점의 index 구함\n",
        "def each_visualize_comparison(data, index_num, cycle_duration):\n",
        "    start_index = max(index_num - cycle_duration, 0)\n",
        "    end_index = min(index_num + cycle_duration, len(data))\n",
        "    selected_data = data.iloc[start_index:end_index].copy()\n",
        "\n",
        "    # 날짜 열 값 가져오기\n",
        "    date_value = data.loc[index_num, '날짜']\n",
        "\n",
        "    plt.figure(figsize=(30, 8))\n",
        "    plt.plot(selected_data.index, selected_data['VALUE'])\n",
        "    plt.axvline(index_num, color='red', linestyle='--', linewidth=1)\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('VALUE')\n",
        "    plt.title(date_value)\n",
        "    plt.show()\n",
        "\n",
        "    return start_index, end_index\n",
        "\n",
        "\n",
        "\n",
        "# dtw 그래프 그리기\n",
        "def plot_dtw_comparison(data, graph_indices, title):\n",
        "    selected_graphs = []\n",
        "    graph_labels = []\n",
        "    dtw_distances = []\n",
        "\n",
        "    for start_index, end_index in graph_indices:\n",
        "        graph = data.iloc[start_index:end_index]['VALUE'].values\n",
        "        selected_graphs.append(graph)\n",
        "        start_time = data.iloc[start_index]['날짜'].strftime('%H:%M:%S')\n",
        "        end_time = data.iloc[end_index-1]['날짜'].strftime('%H:%M:%S')\n",
        "        graph_labels.append(f'{start_time}-{end_time}')\n",
        "\n",
        "    for i in range(len(selected_graphs)):\n",
        "        for j in range(i+1, len(selected_graphs)):\n",
        "            graph_i = selected_graphs[i]\n",
        "            graph_j = selected_graphs[j]\n",
        "            distance, _ = fastdtw(graph_i, graph_j)\n",
        "            dtw_distances.append((i, j, distance))\n",
        "\n",
        "    dtw_distances.sort(key=lambda x: x[2])  # 거리를 기준으로 정렬\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(15, 6))\n",
        "    for i, j, distance in dtw_distances:\n",
        "        graph_i = selected_graphs[i]\n",
        "        graph_j = selected_graphs[j]\n",
        "        label_i = graph_labels[i]\n",
        "        label_j = graph_labels[j]\n",
        "        if label_i not in ax.get_legend_handles_labels()[1]:\n",
        "            ax.plot(graph_i, label=label_i)\n",
        "        if label_j not in ax.get_legend_handles_labels()[1]:\n",
        "            ax.plot(graph_j, label=label_j)\n",
        "        print(f'DTW Distance between {label_i} and {label_j}: {distance}')\n",
        "\n",
        "    average_distance = sum(distance for _, _, distance in dtw_distances) / len(dtw_distances)\n",
        "    print(\"Average DTW Distance:\", average_distance)\n",
        "\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('VALUE')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "dEt2I2PzqJGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "l9N7AgBlqP5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# poor_weld\n",
        "poor_weld.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
        "poor_weld_column_names = list(poor_weld.iloc[0])\n",
        "poor_weld = poor_weld[1:]\n",
        "poor_weld.columns = poor_weld_column_names\n",
        "\n",
        "# 데이터프레임 전치\n",
        "poor_weld_trans = poor_weld.transpose()\n",
        "\n",
        "poor_weld_trans.columns = poor_weld_trans.iloc[0]\n",
        "poor_weld_trans = poor_weld_trans.iloc[1:-1]\n",
        "# 불량수량과 생산수량이 모두 0이 아닌 경우에만 불량비율 계산\n",
        "mask = (poor_weld_trans['불량수량'] != 0) & (poor_weld_trans['생산수량'] != 0)\n",
        "poor_weld_trans.loc[mask, '불량비율'] = poor_weld_trans.loc[mask, '불량수량'] / poor_weld_trans.loc[mask, '생산수량']\n",
        "\n",
        "# 불량수량이 0인 경우 불량비율을 0으로 설정\n",
        "poor_weld_trans.loc[poor_weld_trans['불량수량'] == 0, '불량비율'] = 0\n",
        "poor_weld_trans.index = pd.to_datetime(poor_weld_trans.index, format='%Y-%m-%d')\n",
        "\n",
        "poor_weld_trans.reset_index(inplace=True)\n",
        "poor_weld_trans.rename(columns={'index': '일자'}, inplace=True)\n",
        "\n",
        "# time_model\n",
        "time_model_2.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
        "time_model_2_column_names = list(time_model_2.iloc[0])\n",
        "time_model_2 = time_model_2[1:]\n",
        "time_model_2.columns = time_model_2_column_names\n",
        "\n",
        "time_model_3.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
        "time_model_3_column_names = list(time_model_3.iloc[0])\n",
        "time_model_3 = time_model_3[1:]\n",
        "time_model_3.columns = time_model_3_column_names\n",
        "\n",
        "# weld_strength\n",
        "weld_strength.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
        "weld_strength.columns = weld_strength.iloc[0]\n",
        "weld_strength = weld_strength[1:].reset_index(drop=True)\n",
        "\n",
        "new_columns = []\n",
        "for column in weld_strength.columns:\n",
        "    if column == '용착강도(좌)':\n",
        "        new_columns.extend([f'용착강도(좌)_{i}' for i in range(1, 9)])\n",
        "    elif column == '용착강도(우)':\n",
        "        new_columns.extend([f'용착강도(우)_{i}' for i in range(1, 9)])\n",
        "    elif pd.notnull(column):\n",
        "        new_columns.append(column)\n",
        "\n",
        "weld_strength.columns = new_columns\n",
        "weld_strength = weld_strength.iloc[1:]\n",
        "\n",
        "# '용착강도(좌)'와 '용착강도(우)'를 포함하는 행 제거\n",
        "weld_strength = weld_strength[~weld_strength.isin(['용착강도(좌)', '용착강도(우)']).any(axis=1)]\n",
        "weld_strength = weld_strength.dropna(subset=['일자', '시간', '수거시간', 'EU,US'], how='all')\n",
        "weld_strength = weld_strength.fillna(method='ffill')\n",
        "weld_strength = weld_strength.reset_index(drop=True)\n",
        "\n",
        "# Replace 'X' with NaN\n",
        "weld_strength.replace('X', np.nan, inplace=True)\n",
        "\n",
        "weld_strength_filtered = weld_strength.dropna(subset=['수거시간', 'EU,US', '용착강도(좌)_1', '용착강도(좌)_2', '용착강도(좌)_3', '용착강도(좌)_4', '용착강도(좌)_5', '용착강도(좌)_6', '용착강도(좌)_7', '용착강도(좌)_8',\n",
        "                                                       '용착강도(우)_1', '용착강도(우)_2', '용착강도(우)_3', '용착강도(우)_4', '용착강도(우)_5', '용착강도(우)_6', '용착강도(우)_7', '용착강도(우)_8'], how='all')\n",
        "weld_strength_filtered = weld_strength_filtered.reset_index(drop=True)\n",
        "\n",
        "# Concatenate '일자' and '수거시간' columns\n",
        "combined_datetime = weld_strength_filtered['일자'].astype(str) + ' ' + weld_strength_filtered['수거시간'].astype(str)\n",
        "weld_strength_filtered['날짜'] = pd.to_datetime(combined_datetime, format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "\n",
        "# 전류 데이터 전처리\n",
        "def Clean(df):\n",
        "    df = df.drop(df.columns[0], axis=1)\n",
        "    new_column_name = df.iloc[2, :]\n",
        "    new_column_name = new_column_name.reset_index(drop=True)\n",
        "    df.columns = new_column_name\n",
        "    df = df.iloc[3:]\n",
        "    df = df.drop(\"TAG DESC\", axis=1)\n",
        "    df['날짜'] = pd.to_datetime(df['날짜'])\n",
        "    df['VALUE'] = pd.to_numeric(df['VALUE'])  # VALUE 열의 데이터 타입을 숫자로 변환\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "qmY_dOxKqRIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_04_01_04_08 = Clean(avg_04_01_04_08)\n",
        "avg_04_10_04_15 = Clean(avg_04_10_04_15)\n",
        "avg_04_17_04_22 = Clean(avg_04_17_04_22)\n",
        "avg_04_24_05_02 = Clean(avg_04_24_05_02)\n",
        "avg_05_17 = Clean(avg_05_17)\n",
        "avg_05_18 = Clean(avg_05_18)\n",
        "avg_05_19 = Clean(avg_05_19)\n",
        "avg_05_25 = Clean(avg_05_25)\n",
        "avg_05_26 = Clean(avg_05_26)\n",
        "avg_05_27 = Clean(avg_05_27)\n",
        "avg_05_29 = Clean(avg_05_29)\n",
        "avg_05_30 = Clean(avg_05_30)\n",
        "avg_05_31 = Clean(avg_05_31)\n",
        "avg_06_01 = Clean(avg_06_01)\n",
        "avg_06_02 = Clean(avg_06_02)\n",
        "avg_06_03 = Clean(avg_06_03)\n",
        "\n",
        "avg_4 = pd.concat([avg_04_01_04_08, avg_04_10_04_15, avg_04_17_04_22, avg_04_24_05_02])\n",
        "avg_4 = avg_4.reset_index(drop=True)\n",
        "avg_5_6 = pd.concat([avg_05_17, avg_05_18, avg_05_19, avg_05_25, avg_05_26, avg_05_27, avg_05_29, avg_05_30, avg_05_31, avg_06_01, avg_06_02, avg_06_03 ])\n",
        "avg_5_6 = avg_5_6.reset_index(drop=True)\n",
        "avg_all = pd.concat([avg_4,avg_5_6])\n",
        "avg_all = avg_all.reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "avg_08_07 = Clean(avg_08_07)"
      ],
      "metadata": {
        "id": "1g9hdCYjqbEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Current_Strength_data = weld_strength_filtered.merge(avg_all, on='날짜', how='right')\n",
        "Current_Strength_data = poor_weld_trans.merge(Current_Strength_data, on='일자', how='right')\n",
        "\n",
        "cols = ['날짜', 'VALUE'] + [col for col in Current_Strength_data.columns if col not in ['날짜', 'VALUE']]\n",
        "Current_Strength_data = Current_Strength_data[cols]"
      ],
      "metadata": {
        "id": "yf1jE_p-qdPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '용착강도(좌)_1'부터 '용착강도(우)_8'까지 열 선택\n",
        "columns = ['용착강도(좌)_1', '용착강도(좌)_2', '용착강도(좌)_3', '용착강도(좌)_4', '용착강도(좌)_5',\n",
        "           '용착강도(좌)_6', '용착강도(좌)_7', '용착강도(좌)_8', '용착강도(우)_1', '용착강도(우)_2',\n",
        "           '용착강도(우)_3', '용착강도(우)_4', '용착강도(우)_5', '용착강도(우)_6', '용착강도(우)_7',\n",
        "           '용착강도(우)_8']\n",
        "\n",
        "# Extract the weld strength columns for different sections\n",
        "weld_strength_left = Current_Strength_data[['용착강도(좌)_1', '용착강도(좌)_2', '용착강도(좌)_3', '용착강도(좌)_4', '용착강도(좌)_5', '용착강도(좌)_6','용착강도(좌)_7', '용착강도(좌)_8']]\n",
        "weld_strength_right = Current_Strength_data[['용착강도(우)_1', '용착강도(우)_2', '용착강도(우)_3', '용착강도(우)_4', '용착강도(우)_5', '용착강도(우)_6','용착강도(우)_7', '용착강도(우)_8']]\n",
        "\n",
        "\n",
        "Current_Strength_data['Min_Max_diff'] =  Current_Strength_data[columns].max(axis=1) -  Current_Strength_data[columns].min(axis=1)\n",
        "Current_Strength_data['weld_strength_left_mean'] = weld_strength_left.mean(axis=1)\n",
        "Current_Strength_data['weld_strength_right_mean'] = weld_strength_right.mean(axis=1)\n",
        "Current_Strength_data['VALUE_DIFF'] = Current_Strength_data['VALUE'].diff().fillna(0)\n",
        "\n",
        "\n",
        "time_ranges = [('2023-05-17 08:00:00', '2023-05-17 10:59:59', 'EU'),\n",
        "               ('2023-05-17 11:00:00', '2023-05-17 22:59:59', 'US'),\n",
        "               ('2023-05-17 23:00:00', '2023-05-17 23:59:59', 'EU'),\n",
        "               ('2023-05-18 01:00:00', '2023-05-18 06:59:59', 'EU'),\n",
        "               ('2023-05-18 08:00:00', '2023-05-18 23:29:59', 'US'),\n",
        "               ('2023-05-18 23:30:00', '2023-05-19 05:29:59', 'EU'),\n",
        "               ('2023-05-19 05:30:00', '2023-05-19 09:29:59', 'US'),\n",
        "               ('2023-05-19 20:00:00', '2023-05-20 00:29:59', 'US'),\n",
        "               ('2023-05-20 00:30:00', '2023-05-20 06:59:59', 'EU'),\n",
        "               ('2023-05-25 08:00:00', '2023-05-25 10:59:59', 'US'),\n",
        "               ('2023-05-25 11:00:00', '2023-05-25 13:59:59', 'EU'),\n",
        "               ('2023-05-25 15:00:00', '2023-05-25 16:59:59', 'EU'),\n",
        "               ('2023-05-25 17:00:00', '2023-05-25 18:59:59', 'US'),\n",
        "               ('2023-05-25 22:00:00', '2023-05-25 23:59:59', 'US'),\n",
        "               ('2023-05-26 01:00:00', '2023-05-26 05:59:59', 'EU'),\n",
        "               ('2023-05-26 08:00:00', '2023-05-26 14:59:59', 'EU'),\n",
        "               ('2023-05-26 15:00:00', '2023-05-26 23:59:59', 'US'),\n",
        "               ('2023-05-27 01:00:00', '2023-05-27 06:59:59', 'US'),\n",
        "               ('2023-05-29 08:00:00', '2023-05-29 10:29:59', 'US'),\n",
        "               ('2023-05-29 10:30:00', '2023-05-29 22:59:59', 'EU'),\n",
        "               ('2023-05-30 03:00:00', '2023-05-30 06:59:59', 'US'),\n",
        "               ('2023-05-30 08:00:00', '2023-05-30 15:59:59', 'EU'),\n",
        "               ('2023-05-30 16:00:00', '2023-05-30 23:59:59', 'US'),\n",
        "               ('2023-05-31 01:00:00', '2023-05-31 06:59:59', 'US'),\n",
        "               ('2023-05-31 08:00:00', '2023-05-31 16:59:59', 'EU'),\n",
        "               ('2023-05-31 20:00:00', '2023-05-31 21:59:59', 'US'),\n",
        "               ('2023-06-01 05:00:00', '2023-06-01 15:59:59', 'US'),\n",
        "               ('2023-06-01 16:00:00', '2023-06-01 21:59:59', 'EU'),\n",
        "               ('2023-06-01 22:00:00', '2023-06-01 23:59:59', 'US'),\n",
        "               ('2023-06-02 01:00:00', '2023-06-02 04:59:59', 'US'),\n",
        "               ('2023-06-02 11:00:00', '2023-06-02 23:59:59', 'US'),\n",
        "               ('2023-06-03 01:00:00', '2023-06-03 01:59:59', 'US'),\n",
        "               ('2023-06-03 02:00:00', '2023-06-03 06:59:59', 'EU')]\n",
        "\n",
        "Current_Strength_US_EU_data = change_category_by_time_range(Current_Strength_data, time_ranges)\n",
        "\n",
        "Current_Strength_US_EU_data['strength_mean'] = Current_Strength_US_EU_data[['용착강도(좌)_1', '용착강도(좌)_2', '용착강도(좌)_3', '용착강도(좌)_4', '용착강도(좌)_5',\n",
        "                        '용착강도(좌)_6', '용착강도(좌)_7', '용착강도(좌)_8', '용착강도(우)_1', '용착강도(우)_2',\n",
        "                        '용착강도(우)_3', '용착강도(우)_4', '용착강도(우)_5', '용착강도(우)_6', '용착강도(우)_7',\n",
        "                        '용착강도(우)_8']].mean(axis=1)"
      ],
      "metadata": {
        "id": "Qh2ipqg3qfhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Current_Strength_US_EU_data"
      ],
      "metadata": {
        "id": "pFfVKn5xqgxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 시각화"
      ],
      "metadata": {
        "id": "1akp0VO_WlV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 용착강도 값 분포"
      ],
      "metadata": {
        "id": "a_BHOrqCWcBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = Current_Strength_US_EU_data[Current_Strength_US_EU_data['날짜'].isin(weld_strength_filtered['날짜'])]"
      ],
      "metadata": {
        "id": "X-lFwZPGWcZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수치 값들을 저장할 리스트\n",
        "all_value = []\n",
        "\n",
        "# 열들을 순회하면서 수치 값들을 가져와 리스트에 추가\n",
        "for i in ['용착강도(좌)_1', '용착강도(좌)_2', '용착강도(좌)_3', '용착강도(좌)_4', '용착강도(좌)_5',\n",
        "            '용착강도(좌)_6', '용착강도(좌)_7', '용착강도(좌)_8', '용착강도(우)_1', '용착강도(우)_2',\n",
        "            '용착강도(우)_3', '용착강도(우)_4', '용착강도(우)_5', '용착강도(우)_6', '용착강도(우)_7',\n",
        "            '용착강도(우)_8']:\n",
        "    value = weld_strength_filtered[i].values\n",
        "    all_value.extend(value)\n",
        "\n",
        "# 수치 값들의 개수를 세기\n",
        "value_count = pd.Series(all_value).value_counts()\n",
        "\n",
        "# 시각화\n",
        "plt.bar(value_count.index, value_count.values)\n",
        "plt.xlabel('strengths')\n",
        "plt.ylabel('count')\n",
        "plt.title('Strengths_distribution')\n",
        "# 눈금 간격 1로 변경(x, y축 모두)\n",
        "plt.xticks(range(30))\n",
        "plt.yticks(range(10))\n",
        "plt.grid()\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TklTRq8YWeFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 match 된 것\n",
        "for_plot = merged_data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "eMiN9H3BWfxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수치 값들을 저장할 리스트\n",
        "all_values = []\n",
        "\n",
        "# 열들을 순회하면서 수치 값들을 가져와 리스트에 추가\n",
        "for col in ['용착강도(좌)_1', '용착강도(좌)_2', '용착강도(좌)_3', '용착강도(좌)_4', '용착강도(좌)_5',\n",
        "            '용착강도(좌)_6', '용착강도(좌)_7', '용착강도(좌)_8', '용착강도(우)_1', '용착강도(우)_2',\n",
        "            '용착강도(우)_3', '용착강도(우)_4', '용착강도(우)_5', '용착강도(우)_6', '용착강도(우)_7',\n",
        "            '용착강도(우)_8']:\n",
        "    values = for_plot [col].values\n",
        "    all_values.extend(values)\n",
        "\n",
        "# 수치 값들의 개수를 세기\n",
        "value_counts = pd.Series(all_values).value_counts()\n",
        "\n",
        "# 시각화\n",
        "plt.bar(value_counts.index, value_counts.values)\n",
        "plt.xlabel('strengths')\n",
        "plt.ylabel('count')\n",
        "plt.title('Strengths_distribution')\n",
        "# 눈금 간격 1로 변경(x, y축 모두)\n",
        "plt.xticks(range(30))\n",
        "plt.yticks(range(10))\n",
        "plt.grid()\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jD9w7isLWguS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 제품 원산지에 따른 비교"
      ],
      "metadata": {
        "id": "7IzfzcbRWkHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.boxplot(x='Category', y='VALUE', data=Current_Strength_US_EU_data)\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('VALUE')\n",
        "plt.title('Change in VALUE by Category')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cCUtRdRHWizC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Value 분포"
      ],
      "metadata": {
        "id": "KmAwgQoMWpC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(50,8))\n",
        "plt.plot(Current_Strength_US_EU_data['날짜'], Current_Strength_US_EU_data['VALUE'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('VALUE')\n",
        "plt.title('Trend of VALUE')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fzDxmoGSWoXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Peak 분석"
      ],
      "metadata": {
        "id": "48fgeSzqYFr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prominences: 각 피크의 prominence(눈에 띄는 정도)를 나타내는 1차원 배열\n",
        "# width_heights: 각 피크의 너비에 해당하는 높이를 나타내는 1차원 배열\n",
        "# widths: 각 피크의 너비를 나타내는 1차원 배열\n",
        "# peak_heights: 각 피크의 높이를 나타내는 1차원 배열"
      ],
      "metadata": {
        "id": "MdCAVQ46YUuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 용착 강도가 존재하는 부분에 대해 5월 26일(EU) Peak 분석"
      ],
      "metadata": {
        "id": "U5cfJrslWoh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용 X\n",
        "# EU 제품\n",
        "between_8_43_30_EU_data = extract_data_between_time(data, '2023-05-26 8:40:00', '2023-05-26 8:50:00', 'VALUE')\n",
        "between_9_11_34_EU_data = extract_data_between_time(data, '2023-05-26 9:10:00', '2023-05-26 9:20:00', 'VALUE')\n",
        "between_10_18_07_EU_data = extract_data_between_time(data, '2023-05-26 10:10:00', '2023-05-26 10:20:00', 'VALUE')\n",
        "between_11_04_09_EU_data = extract_data_between_time(data, '2023-05-26 11:00:00', '2023-05-26 11:10:00', 'VALUE')\n",
        "between_13_11_42_EU_data = extract_data_between_time(data, '2023-05-26 13:10:00', '2023-05-26 13:20:00', 'VALUE')\n",
        "between_14_08_09_EU_data = extract_data_between_time(data, '2023-05-26 14:00:00', '2023-05-26 14:10:00', 'VALUE')\n",
        "\n",
        "# US 제품\n",
        "between_15_24_55_US_data = extract_data_between_time(data, '2023-05-26 15:20:00', '2023-05-26 15:30:00', 'VALUE')\n",
        "between_15_55_21_US_data = extract_data_between_time(data, '2023-05-26 15:50:00', '2023-05-26 16:00:00', 'VALUE')\n",
        "between_16_08_32_US_data = extract_data_between_time(data, '2023-05-26 16:00:00', '2023-05-26 16:10:00', 'VALUE')\n",
        "\n",
        "### 사용 O\n",
        "Data_05_26 = Current_Strength_US_EU_data[Current_Strength_US_EU_data['날짜'].dt.date == pd.to_datetime('2023-05-26').date()]\n",
        "\n",
        "filtered_data_05_26_us = filter_rows_with_values(Data_05_26, 'US')\n",
        "filtered_data_05_26_eu = filter_rows_with_values(Data_05_26, 'EU')\n",
        "\n",
        "data_05_26_us = filtered_data_05_26_us[['날짜','VALUE','용착강도(좌)_1', '용착강도(좌)_2', '용착강도(좌)_3', '용착강도(좌)_4', '용착강도(좌)_5',\n",
        "                        '용착강도(좌)_6', '용착강도(좌)_7', '용착강도(좌)_8', '용착강도(우)_1', '용착강도(우)_2',\n",
        "                        '용착강도(우)_3', '용착강도(우)_4', '용착강도(우)_5', '용착강도(우)_6', '용착강도(우)_7',\n",
        "                        '용착강도(우)_8']]\n",
        "data_05_26_eu = filtered_data_05_26_eu[['날짜','VALUE','용착강도(좌)_1', '용착강도(좌)_2', '용착강도(좌)_3', '용착강도(좌)_4', '용착강도(좌)_5',\n",
        "                        '용착강도(좌)_6', '용착강도(좌)_7', '용착강도(좌)_8', '용착강도(우)_1', '용착강도(우)_2',\n",
        "                        '용착강도(우)_3', '용착강도(우)_4', '용착강도(우)_5', '용착강도(우)_6', '용착강도(우)_7',\n",
        "                        '용착강도(우)_8']]"
      ],
      "metadata": {
        "id": "wDZogLu9YMwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26일에 용착 강도가 존재하는 부분에 대해서만 10분 간격으로 Peak 데이터 시각화\n",
        "time_ranges = [('8_43_30', '8:40:00', '8:50:00'),\n",
        "               ('9_11_34', '9:10:00', '9:20:00'),\n",
        "               ('10_18_07', '10:10:00', '10:20:00'),\n",
        "               ('11_04_09', '11:00:00', '11:10:00'),\n",
        "               ('13_11_42', '13:10:00', '13:20:00'),\n",
        "               ('14_08_09', '14:00:00', '14:10:00')]\n",
        "\n",
        "peaks_data = []\n",
        "properties_data = []\n",
        "value_data = []\n",
        "\n",
        "for data_name, start_time, end_time in time_ranges:\n",
        "    between_time_data = extract_data_between_time(data, f'2023-05-26 {start_time}', f'2023-05-26 {end_time}', 'VALUE')\n",
        "    between_time_data = between_time_data.flatten()\n",
        "    peaks, properties = find_peaks(between_time_data, prominence=2, width=2, height=5)\n",
        "\n",
        "    # Store peaks in individual variables\n",
        "    exec(f\"peaks_{data_name} = peaks\")\n",
        "\n",
        "    # Store properties in individual variables\n",
        "    exec(f\"properties_{data_name} = properties\")\n",
        "\n",
        "    # Store peaks and properties in lists\n",
        "    peaks_data.append(peaks)\n",
        "    properties_data.append(properties)\n",
        "\n",
        "    # Store original VALUE values corresponding to the peaks\n",
        "    value_data.append(between_time_data[peaks])\n",
        "\n",
        "    plt.figure(figsize=(50, 10))\n",
        "    plt.plot(between_time_data)\n",
        "    plt.plot(peaks, between_time_data[peaks], \"x\")\n",
        "    plt.plot(np.zeros_like(between_time_data), \"--\", color=\"gray\")\n",
        "    plt.title(f'Peaks at {data_name}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "2Nh_0Iy_YNa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5월 26일 전체 EU Peaks"
      ],
      "metadata": {
        "id": "yC-rM8tAYZdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Current_Strength_EU_data = Current_Strength_US_EU_data[(Current_Strength_US_EU_data['날짜'].dt.date == pd.to_datetime('2023-05-26').date()) & (Current_Strength_US_EU_data['Category'] == 'EU')]\n",
        "All_5_26_EU_data = filter_data_by_date_and_category(Current_Strength_US_EU_data, '2023-05-26', 'VALUE', 'EU')"
      ],
      "metadata": {
        "id": "lHwdN9nHYZDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5월 26일 EU 전력 데이터 시각화\n",
        "peaks_EU_5_26, properties_EU_5_26 = find_peaks(All_5_26_EU_data, prominence=5,width=2,height=10)\n",
        "plt.figure(figsize=(50,10))\n",
        "plt.plot(All_5_26_EU_data)\n",
        "plt.plot(peaks_EU_5_26, All_5_26_EU_data[peaks_EU_5_26], \"x\")\n",
        "#plt.plot(np.zeros_like(between_8_43_30_EU_data), \"--\", color=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CbUknQD-Yb8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "axs[0, 0].hist(properties_EU_5_26[\"prominences\"], bins=5, alpha=0.5, label='0526')\n",
        "axs[0, 0].set_xlabel('prominences number')\n",
        "axs[0, 0].set_ylabel('count')\n",
        "\n",
        "axs[0, 1].hist(properties_EU_5_26[\"width_heights\"], alpha=0.5, label='0526')\n",
        "axs[0, 1].set_xlabel('width_heights number')\n",
        "axs[0, 1].set_ylabel('count')\n",
        "\n",
        "axs[1, 0].hist(properties_EU_5_26[\"widths\"], alpha=0.5, label='0526')\n",
        "axs[1, 0].set_xlabel('widths number')\n",
        "axs[1, 0].set_ylabel('count')\n",
        "\n",
        "axs[1, 1].hist(properties_EU_5_26[\"peak_heights\"], alpha=0.5, label='0526')\n",
        "axs[1, 1].set_xlabel('peak_heights number')\n",
        "axs[1, 1].set_ylabel('count')\n",
        "\n",
        "plt.tight_layout()\n",
        "#fig.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mARF9eiRYdrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5월 6일 EU기기 데이터 중 \"prominences\" 값이 상위 90%에 해당하는 숫자고른다.\n",
        "# prominence 값이 클수록 해당 피크가 주변 신호와 뚜렷하게 구분되는 정도가 높다는 의미이다.\n",
        "# 숫자 값을 바탕으로 원래 데이터셋의 어느 위치에 해당하는지 추출: 단, 숫자 바탕으로 탐색했기에 추출 숫자 개수보다 많이 출력이 가능하다."
      ],
      "metadata": {
        "id": "0LSWO7ICYfVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_EU = combine_selected_data(All_5_26_EU_data, Current_Strength_EU_data, peaks_EU_5_26, properties_EU_5_26, 99)\n",
        "combined_EU_Peak = combined_EU.sort_index()\n",
        "\n",
        "duplicated_EU_Peak = combined_EU_Peak[combined_EU_Peak.duplicated(subset='날짜', keep=False)]\n",
        "EU_Peak = duplicated_EU_Peak.drop_duplicates(subset='날짜')"
      ],
      "metadata": {
        "id": "iMYPrcXeYgPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5월 26일 전체 US Peak"
      ],
      "metadata": {
        "id": "A7oqDiXjYj95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Current_Strength_US_data = Current_Strength_US_EU_data[(Current_Strength_US_EU_data['날짜'].dt.date == pd.to_datetime('2023-05-26').date()) & (Current_Strength_US_EU_data['Category'] == 'US')]\n",
        "All_5_26_US_data = filter_data_by_date_and_category(Current_Strength_US_EU_data, '2023-05-26', 'VALUE', 'US')"
      ],
      "metadata": {
        "id": "LPLDA2clYmF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peaks_US_5_26, properties_US_5_26 = find_peaks(All_5_26_US_data, prominence=5,width=2,height=10)\n",
        "plt.figure(figsize=(50,10))\n",
        "plt.plot(All_5_26_US_data)\n",
        "plt.plot(peaks_US_5_26, All_5_26_US_data[peaks_US_5_26], \"x\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e6VS4BWEYot7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "axs[0, 0].hist(properties_US_5_26[\"prominences\"], bins=5, alpha=0.5, label='0526')\n",
        "axs[0, 0].set_xlabel('prominences number')\n",
        "axs[0, 0].set_ylabel('count')\n",
        "\n",
        "axs[0, 1].hist(properties_US_5_26[\"width_heights\"], alpha=0.5, label='0526')\n",
        "axs[0, 1].set_xlabel('width_heights number')\n",
        "axs[0, 1].set_ylabel('count')\n",
        "\n",
        "axs[1, 0].hist(properties_US_5_26[\"widths\"], alpha=0.5, label='0526')\n",
        "axs[1, 0].set_xlabel('widths number')\n",
        "axs[1, 0].set_ylabel('count')\n",
        "\n",
        "axs[1, 1].hist(properties_US_5_26[\"peak_heights\"], alpha=0.5, label='0526')\n",
        "axs[1, 1].set_xlabel('peak_heights number')\n",
        "axs[1, 1].set_ylabel('count')\n",
        "\n",
        "plt.tight_layout()\n",
        "#fig.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dn5StUkJYp4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_US = combine_selected_data(All_5_26_US_data, Current_Strength_US_data, peaks_US_5_26, properties_US_5_26, 99)\n",
        "combined_US_Peak = combined_US.sort_index()\n",
        "\n",
        "duplicated_US_Peak = combined_US_Peak[combined_US_Peak.duplicated(subset='날짜', keep=False)]\n",
        "US_Peak = duplicated_US_Peak.drop_duplicates(subset='날짜')"
      ],
      "metadata": {
        "id": "mswzVZEsYrKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### US,EU Peak 데이터 시각화"
      ],
      "metadata": {
        "id": "ZbqmQwT2YtK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "US_graph_indices = []\n",
        "\n",
        "for index, row in US_Peak.iterrows():\n",
        "    start_index, end_index = each_visualize_comparison(Current_Strength_US_EU_data, index, 5)\n",
        "    US_graph_indices.append((start_index, end_index))"
      ],
      "metadata": {
        "id": "hp2RYfwkuxkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EU_graph_indices = []\n",
        "\n",
        "for index, row in EU_Peak.iterrows():\n",
        "    start_index, end_index = each_visualize_comparison(Current_Strength_US_EU_data, index, 5)\n",
        "    EU_graph_indices.append((start_index, end_index))"
      ],
      "metadata": {
        "id": "mN6yGIZeuyvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# US 그래프 그리기\n",
        "plot_dtw_comparison(Current_Strength_US_EU_data, US_graph_indices, 'US DTW Comparison of Selected Graphs')"
      ],
      "metadata": {
        "id": "dqkVzsmKu0t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EU 그래프 그리기\n",
        "plot_dtw_comparison(Current_Strength_US_EU_data, EU_graph_indices, 'EU DTW Comparison of Selected Graphs')"
      ],
      "metadata": {
        "id": "fT59ijocu1sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MIN-MAX 기준 Peak 분석"
      ],
      "metadata": {
        "id": "q2QFPjxAu7PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dates = ['2023-05-17', '2023-05-18', '2023-05-19','2023-05-22','2023-05-23','2023-05-24',\n",
        "         '2023-05-25','2023-05-26','2023-05-29','2023-05-30','2023-05-31','2023-06-01','2023-06-02']\n",
        "selected_dates = pd.to_datetime(dates).date\n",
        "\n",
        "Data_05_06_strength = Current_Strength_US_EU_data[Current_Strength_US_EU_data['날짜'].dt.date.isin(selected_dates)]\n",
        "\n",
        "Data_05_06_strength = Data_05_06_strength.set_index('날짜')\n",
        "\n",
        "filtered_data_strength_us = filter_rows_with_values(Data_05_06_strength , 'US')\n",
        "filtered_data_strength_eu = filter_rows_with_values(Data_05_06_strength , 'EU')\n",
        "\n",
        "strength_us = filtered_data_strength_us[['VALUE','Min_Max_diff']]\n",
        "strength_eu = filtered_data_strength_eu[['VALUE','Min_Max_diff']]"
      ],
      "metadata": {
        "id": "0g6YH4Ipu6vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# US 데이터\n",
        "num_bins = 5\n",
        "\n",
        "min_val = strength_us['Min_Max_diff'].min()\n",
        "max_val = strength_us['Min_Max_diff'].max()\n",
        "\n",
        "bin_size = (max_val - min_val) / num_bins\n",
        "\n",
        "strength_us['Min_Max_diff_bin'] = pd.cut(strength_us['Min_Max_diff'], bins=num_bins, labels=False, include_lowest=True)\n",
        "\n",
        "# 3.898 / 6.986 / 10.074 / 13.162\n"
      ],
      "metadata": {
        "id": "QL_RiA_Fu-3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(strength_us['Min_Max_diff_bin'], bins=num_bins, edgecolor='black', alpha=0.5)\n",
        "plt.xlabel('Min_Max_diff_bin')\n",
        "plt.ylabel('Count')\n",
        "plt.title('US Distribution of Min_Max_diff_bin')\n",
        "plt.xticks(range(num_bins))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ASl36sa2vCd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EU 데이터\n",
        "num_bins = 5\n",
        "\n",
        "min_val = strength_eu['Min_Max_diff'].min()\n",
        "max_val = strength_eu['Min_Max_diff'].max()\n",
        "\n",
        "bin_size = (max_val - min_val) / num_bins\n",
        "\n",
        "strength_eu['Min_Max_diff_bin'] = pd.cut(strength_eu['Min_Max_diff'], bins=num_bins, labels=False, include_lowest=True)\n",
        "# 3.65 / 6.294 / 8.936 / 11.578\n"
      ],
      "metadata": {
        "id": "PqVaHI5Cu_4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(strength_eu['Min_Max_diff_bin'], bins=num_bins, edgecolor='black', alpha=0.5)\n",
        "plt.xlabel('Min_Max_diff_bin')\n",
        "plt.ylabel('Count')\n",
        "plt.title('EU Distribution of Min_Max_diff_bin')\n",
        "plt.xticks(range(num_bins))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EO8gmodUvDlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Current_Strength_US_EU_data.reset_index(drop=True, inplace=True)\n",
        "Current_Strength_US_EU_index_data = Current_Strength_US_EU_data.set_index('날짜')"
      ],
      "metadata": {
        "id": "DsdVp4F3vBG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import datetime\n",
        "from scipy.signal import find_peaks, peak_widths\n",
        "\n",
        "\n",
        "def extract_data_between_time(data, start_time, end_time, column_name):\n",
        "    start_time = pd.to_datetime(start_time)\n",
        "    end_time = pd.to_datetime(end_time)\n",
        "    between_time_data = data[(data.index < end_time) & (data.index >= start_time)]\n",
        "    between_time_data = between_time_data[[column_name]]\n",
        "    between_time_data = np.array(between_time_data)\n",
        "    return between_time_data\n",
        "\n",
        "def process_strength(data):\n",
        "    peaks_data = []\n",
        "    properties_data = []\n",
        "    widths_data = []\n",
        "    width_heights_data = []\n",
        "    value_data = []\n",
        "\n",
        "    for date_value in data.index:\n",
        "        start_time = date_value - datetime.timedelta(seconds=10)\n",
        "        end_time = date_value + datetime.timedelta(seconds=10)\n",
        "\n",
        "        between_time_data = extract_data_between_time(Current_Strength_US_EU_index_data, start_time, end_time, 'VALUE')\n",
        "        between_time_data = between_time_data.flatten()\n",
        "        peaks, properties = find_peaks(between_time_data, prominence=4, height=5)\n",
        "\n",
        "        widths, width_heights, left_bases, right_bases = peak_widths(between_time_data, peaks)\n",
        "\n",
        "        # Store peaks in individual variables\n",
        "        exec(f\"peaks_{str(index).replace('-', '_').replace(' ', '_').replace(':', '_')} = peaks\")\n",
        "\n",
        "        # Store properties in individual variables\n",
        "        exec(f\"properties_{str(index).replace('-', '_').replace(' ', '_').replace(':', '_')} = properties\")\n",
        "\n",
        "        # Store widths and width heights in individual variables\n",
        "        exec(f\"widths_{str(index).replace('-', '_').replace(' ', '_').replace(':', '_')} = widths\")\n",
        "        exec(f\"width_heights_{str(index).replace('-', '_').replace(' ', '_').replace(':', '_')} = width_heights\")\n",
        "\n",
        "        # Store peaks and properties in lists\n",
        "        peaks_data.append(peaks)\n",
        "        properties_data.append(properties)\n",
        "        widths_data.append(widths)\n",
        "        width_heights_data.append(width_heights)\n",
        "\n",
        "        # Store original Current values corresponding to the peaks\n",
        "        value_data.append(between_time_data[peaks])\n",
        "\n",
        "    return peaks_data, properties_data, widths_data, width_heights_data, value_data\n",
        "\n",
        "\n",
        "for bin_val in range(5):\n",
        "    globals()[f\"strength_us_{bin_val}\"] = strength_us[strength_us['Min_Max_diff_bin'] == bin_val]\n",
        "\n",
        "us_peaks_data_dict = {}\n",
        "us_properties_data_dict = {}\n",
        "us_widths_data_dict = {}\n",
        "us_width_heights_data_dict = {}\n",
        "us_value_data_dict = {}\n",
        "\n",
        "for bin_val in range(5):\n",
        "    data = globals()[f\"strength_us_{bin_val}\"]\n",
        "    peaks_data, properties_data, widths_data, width_heights_data, value_data = process_strength(data)\n",
        "    us_peaks_data_dict[f\"peaks_data_{bin_val}\"] = peaks_data\n",
        "    us_properties_data_dict[f\"properties_data_{bin_val}\"] = properties_data\n",
        "    us_widths_data_dict[f\"widths_data_{bin_val}\"] = widths_data\n",
        "    us_width_heights_data_dict[f\"width_heights_data_{bin_val}\"] = width_heights_data\n",
        "    us_value_data_dict[f\"value_data_{bin_val}\"] = value_data\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "for bin_val in range(5):\n",
        "    properties_data = us_properties_data_dict[f'properties_data_{bin_val}']\n",
        "    widths_data = us_widths_data_dict[f'widths_data_{bin_val}']\n",
        "    width_heights_data = us_width_heights_data_dict[f'width_heights_data_{bin_val}']\n",
        "\n",
        "    # Flatten the nested lists and extract peak_heights and prominences\n",
        "    peak_heights = np.concatenate([d['peak_heights'] for d in properties_data]).flatten()\n",
        "    prominences = np.concatenate([d['prominences'] for d in properties_data]).flatten()\n",
        "    widths_data_flat = np.concatenate(widths_data).flatten()\n",
        "    width_heights_data_flat = np.concatenate(width_heights_data).flatten()\n",
        "\n",
        "    # Plot histogram of peak heights with bin-specific color\n",
        "    axs[0, 0].hist(peak_heights, bins=10, alpha=0.5, label=f'Bin {bin_val}', color=f'C{bin_val}')\n",
        "\n",
        "    # Plot histogram of prominences with bin-specific color\n",
        "    axs[0, 1].hist(prominences, bins=10, alpha=0.5, label=f'Bin {bin_val}', color=f'C{bin_val}')\n",
        "\n",
        "    # Plot histogram of widths with bin-specific color\n",
        "    axs[1, 0].hist(widths_data_flat, bins=10, alpha=0.5, label=f'Bin {bin_val}', color=f'C{bin_val}')\n",
        "\n",
        "    # Plot histogram of width heights with bin-specific color\n",
        "    axs[1, 1].hist(width_heights_data_flat, bins=10, alpha=0.5, label=f'Bin {bin_val}', color=f'C{bin_val}')\n",
        "\n",
        "# Set titles and labels for subplots\n",
        "axs[0, 0].set_title('US Peak Heights')\n",
        "axs[0, 0].set_xlabel('Peak Heights')\n",
        "axs[0, 0].set_ylabel('Frequency')\n",
        "axs[0, 0].legend()\n",
        "\n",
        "axs[0, 1].set_title('US Prominences')\n",
        "axs[0, 1].set_xlabel('Prominences')\n",
        "axs[0, 1].set_ylabel('Frequency')\n",
        "axs[0, 1].legend()\n",
        "\n",
        "axs[1, 0].set_title('US Widths')\n",
        "axs[1, 0].set_xlabel('Widths')\n",
        "axs[1, 0].set_ylabel('Frequency')\n",
        "axs[1, 0].legend()\n",
        "\n",
        "axs[1, 1].set_title('US Heights')\n",
        "axs[1, 1].set_xlabel('Width Heights')\n",
        "axs[1, 1].set_ylabel('Frequency')\n",
        "axs[1, 1].legend()\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "VnmHCxo5vI6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean 기준 Peak 분석"
      ],
      "metadata": {
        "id": "f_bJ2Bf1vSXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strength_mean_eu = filtered_data_strength_eu[['VALUE','strength_mean']]\n",
        "strength_mean_us = filtered_data_strength_us[['VALUE','strength_mean']]"
      ],
      "metadata": {
        "id": "Jco--LlUvPHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strength_mean_eu = strength_mean_eu.sort_values(by='strength_mean', ascending=False)\n",
        "\n",
        "num_bins = 4\n",
        "bin_size = len(strength_mean_eu) // num_bins\n",
        "\n",
        "top_25_percent = int(0.25 * len(strength_mean_eu))\n",
        "top_50_percent = int(0.5 * len(strength_mean_eu))\n",
        "top_75_percent = int(0.75 * len(strength_mean_eu))\n",
        "\n",
        "strength_mean_eu['strength_mean_bin'] = 0\n",
        "strength_mean_eu.iloc[top_75_percent:, -1] = 0\n",
        "strength_mean_eu.iloc[top_50_percent:top_75_percent, -1] = 1\n",
        "strength_mean_eu.iloc[top_25_percent:top_50_percent, -1] = 2\n",
        "strength_mean_eu.iloc[:top_25_percent, -1] = 3\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(strength_mean_eu['strength_mean_bin'], bins=num_bins, edgecolor='black', alpha=0.5)\n",
        "plt.xlabel('mean_bin')\n",
        "plt.ylabel('Count')\n",
        "plt.title('EU Distribution of mean_bin')\n",
        "plt.xticks(range(num_bins))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tX9I4Z62vYlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for bin_val in range(4):\n",
        "    globals()[f\"strength_mean_eu_{bin_val}\"] = strength_mean_eu[strength_mean_eu['strength_mean_bin'] == bin_val]\n",
        "\n",
        "eu_peaks_data_dict = {}\n",
        "eu_properties_data_dict = {}\n",
        "eu_widths_data_dict = {}\n",
        "eu_width_heights_data_dict = {}\n",
        "eu_value_data_dict = {}\n",
        "\n",
        "for bin_val in range(4):\n",
        "    data = globals()[f\"strength_mean_eu_{bin_val}\"]\n",
        "    peaks_data, properties_data, widths_data, width_heights_data, value_data = process_strength(data)\n",
        "    eu_peaks_data_dict[f\"peaks_data_{bin_val}\"] = peaks_data\n",
        "    eu_properties_data_dict[f\"properties_data_{bin_val}\"] = properties_data\n",
        "    eu_widths_data_dict[f\"widths_data_{bin_val}\"] = widths_data\n",
        "    eu_width_heights_data_dict[f\"width_heights_data_{bin_val}\"] = width_heights_data\n",
        "    eu_value_data_dict[f\"value_data_{bin_val}\"] = value_data\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "for bin_val in range(4):\n",
        "    properties_data = eu_properties_data_dict[f'properties_data_{bin_val}']\n",
        "    widths_data = eu_widths_data_dict[f'widths_data_{bin_val}']\n",
        "    width_heights_data = eu_width_heights_data_dict[f'width_heights_data_{bin_val}']\n",
        "\n",
        "    # Flatten the nested lists and extract peak_heights and prominences\n",
        "    peak_heights = np.concatenate([d['peak_heights'] for d in properties_data]).flatten()\n",
        "    prominences = np.concatenate([d['prominences'] for d in properties_data]).flatten()\n",
        "    widths_data_flat = np.concatenate(widths_data).flatten()\n",
        "    width_heights_data_flat = np.concatenate(width_heights_data).flatten()\n",
        "\n",
        "    # Plot histogram of peak heights with bin-specific color\n",
        "    axs[0, 0].hist(peak_heights, bins=10, alpha=0.5, label=f'Bin {bin_val}', color=f'C{bin_val}')\n",
        "\n",
        "    # Plot histogram of prominences with bin-specific color\n",
        "    axs[0, 1].hist(prominences, bins=10, alpha=0.5, label=f'Bin {bin_val}', color=f'C{bin_val}')\n",
        "\n",
        "    # Plot histogram of widths with bin-specific color\n",
        "    axs[1, 0].hist(widths_data_flat, bins=10, alpha=0.5, label=f'Bin {bin_val}', color=f'C{bin_val}')\n",
        "\n",
        "    # Plot histogram of width heights with bin-specific color\n",
        "    axs[1, 1].hist(width_heights_data_flat, bins=10, alpha=0.5, label=f'Bin {bin_val}', color=f'C{bin_val}')\n",
        "\n",
        "# Set titles and labels for subplots\n",
        "axs[0, 0].set_title('EU Peak Heights')\n",
        "axs[0, 0].set_xlabel('Peak Heights')\n",
        "axs[0, 0].set_ylabel('Frequency')\n",
        "axs[0, 0].legend()\n",
        "\n",
        "axs[0, 1].set_title('EU Prominences')\n",
        "axs[0, 1].set_xlabel('Prominences')\n",
        "axs[0, 1].set_ylabel('Frequency')\n",
        "axs[0, 1].legend()\n",
        "\n",
        "axs[1, 0].set_title('EU Widths')\n",
        "axs[1, 0].set_xlabel('Widths')\n",
        "axs[1, 0].set_ylabel('Frequency')\n",
        "axs[1, 0].legend()\n",
        "\n",
        "axs[1, 1].set_title('EU Heights')\n",
        "axs[1, 1].set_xlabel('Width Heights')\n",
        "axs[1, 1].set_ylabel('Frequency')\n",
        "axs[1, 1].legend()\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-9KtvalKvb1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strength_mean_us = strength_mean_us.sort_values(by='strength_mean', ascending=False)\n",
        "\n",
        "num_bins = 4\n",
        "bin_size = len(strength_mean_us) // num_bins\n",
        "\n",
        "top_25_percent = int(0.25 * len(strength_mean_eu))\n",
        "top_50_percent = int(0.5 * len(strength_mean_eu))\n",
        "top_75_percent = int(0.75 * len(strength_mean_eu))\n",
        "\n",
        "strength_mean_us['strength_mean_bin'] = 0\n",
        "strength_mean_us.iloc[top_75_percent:, -1] = 0\n",
        "strength_mean_us.iloc[top_50_percent:top_75_percent, -1] = 1\n",
        "strength_mean_us.iloc[top_25_percent:top_50_percent, -1] = 2\n",
        "strength_mean_us.iloc[:top_25_percent, -1] = 3\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(strength_mean_us['strength_mean_bin'], bins=num_bins, edgecolor='black', alpha=0.5)\n",
        "plt.xlabel('mean_bin')\n",
        "plt.ylabel('Count')\n",
        "plt.title('US Distribution of mean_bin')\n",
        "plt.xticks(range(num_bins))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qhcJqZNjvfuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pycaret"
      ],
      "metadata": {
        "id": "5MHQmoO6vtmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pycaret"
      ],
      "metadata": {
        "id": "LRQtpCv_v0vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pycaret\n",
        "from pycaret.anomaly import *\n",
        "from pycaret.anomaly import AnomalyExperiment\n",
        "pycaret.__version__"
      ],
      "metadata": {
        "id": "x78_wDkBv3bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('C:/Users/Admin/Downloads/Etri/Dishwashers_Anomaly_Detection/dataset.csv')"
      ],
      "metadata": {
        "id": "TnN8kEAvv5wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_05_17_24_strength.drop(['Unnamed: 0'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "K7LhIw1sv91c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dates = ['2023-05-17', '2023-05-18', '2023-05-19','2023-05-22','2023-05-23','2023-05-24']\n",
        "         #,'2023-05-25','2023-05-26','2023-05-29','2023-05-30','2023-05-31','2023-06-01','2023-06-02']\n",
        "selected_dates = pd.to_datetime(dates).date\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "Data_05_17_24_strength = df[df['Date'].dt.date.isin(selected_dates)]\n",
        "\n",
        "#filtered_data_strength_us = filter_rows_with_values(Data_05_06_strength , 'US')\n",
        "#filtered_data_strength_eu = filter_rows_with_values(Data_05_06_strength , 'EU')\n",
        "\n",
        "#strength_us = filtered_data_strength_us[['VALUE','Min_Max_diff']]\n",
        "#strength_eu = filtered_data_strength_eu[['VALUE','Min_Max_diff']]"
      ],
      "metadata": {
        "id": "WMVO4qycv8AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_05_17_24 = Data_05_17_24_strength[['Date','VALUE','Category']]"
      ],
      "metadata": {
        "id": "eykPfo_-v_Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anom = setup(data = Data_05_17_24, verbose = 0, normalize = True)"
      ],
      "metadata": {
        "id": "suER-dK4wBCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models()"
      ],
      "metadata": {
        "id": "5MiaV_VhwB_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "te_pred = np.zeros(Data_05_17_24.shape[0], )\n",
        "preds = []\n",
        "pred=[]"
      ],
      "metadata": {
        "id": "pRJMadEXwC-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models()['Name'].index[:-2]"
      ],
      "metadata": {
        "id": "RwBN6AuuwEFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in ['cluster', 'iforest','knn', 'lof', 'svm'] :\n",
        "\n",
        "    ### 모델 호출\n",
        "    model = create_model(name)\n",
        "\n",
        "    ### 모델 학습\n",
        "    fitting = assign_model(model)\n",
        "\n",
        "    ### 예측값 생성\n",
        "    pred = predict_model(model, Data_05_17_24)['Anomaly']\n",
        "    preds.append(pred.tolist())\n",
        "    te_pred += pred"
      ],
      "metadata": {
        "id": "r_qjh_8nwFP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = te_pred > 4\n",
        "Data_05_17_24['LABEL'] = pred"
      ],
      "metadata": {
        "id": "4dxvQPADwHb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_05_17_24_anomaly_us = Data_05_17_24[(Data_05_17_24['LABEL'] == 1) & (Data_05_17_24['Category'] == 'US') & (Data_05_17_24['VALUE'] >= 5)]\n",
        "Data_05_17_24_anomaly_eu = Data_05_17_24[(Data_05_17_24['LABEL'] == 1) & (Data_05_17_24['Category'] == 'EU') & (Data_05_17_24['VALUE'] >= 5)]"
      ],
      "metadata": {
        "id": "E7YXPixNwKQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_indices = Data_05_17_24_anomaly_eu.index\n",
        "corresponding_rows = Data_05_17_24_strength.loc[anomaly_indices]"
      ],
      "metadata": {
        "id": "bA6aiP9PwLgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot value on y-axis and date on x-axis\n",
        "fig = px.line(Data_05_17_24, x=Data_05_17_24.index, y=\"VALUE\", title='UNSUPERVISED ANOMALY DETECTION', template = 'plotly_dark')\n",
        "# create list of outlier_dates\n",
        "outlier_dates = Data_05_17_24[Data_05_17_24['LABEL'] == 1].index\n",
        "# obtain y value of anomalies to plot\n",
        "y_values = [Data_05_17_24.loc[i]['VALUE'] for i in outlier_dates]\n",
        "fig.add_trace(go.Scatter(x=outlier_dates, y=y_values, mode = 'markers',\n",
        "                name = 'Anomaly',\n",
        "                marker=dict(color='red',size=10)))\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ZlGst66SwSB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_05_17_24_anomaly_us_graph_indices = []\n",
        "\n",
        "for index, row in Data_05_17_24_anomaly_us.iterrows():\n",
        "    start_index, end_index = each_visualize_comparison(Current_Strength_US_EU_data, index, 5)\n",
        "    Data_05_17_24_anomaly_us_graph_indices.append((start_index, end_index))"
      ],
      "metadata": {
        "id": "GIDmu3F4wUxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# US 그래프 그리기\n",
        "plot_dtw_comparison(Current_Strength_US_EU_data, Data_05_17_24_anomaly_us_graph_indices, 'US DTW Comparison of Selected Graphs')"
      ],
      "metadata": {
        "id": "zHDH3Ew4wVvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DTW 유사도로 판단"
      ],
      "metadata": {
        "id": "Glkg8V-9wcQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import find_peaks\n",
        "from tslearn.metrics import dtw\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "# Extract the 'VALUE' series from the DataFrame\n",
        "data = Data_05_17_19_strength_EU['VALUE'].values\n",
        "\n",
        "# Detect peaks\n",
        "peaks, _ = find_peaks(data, prominence=5, width=2, height=10)\n",
        "\n",
        "# Define a window size to capture the cycle (10 seconds before and after peak)\n",
        "window_size = 10\n",
        "\n",
        "# Segment the time series data into individual cycles and record corresponding dates\n",
        "cycles = []\n",
        "cycle_dates = []\n",
        "for peak in peaks:\n",
        "    start_index = max(0, peak - window_size)\n",
        "    end_index = min(len(data), peak + window_size)\n",
        "    cycles.append(data[start_index : end_index])\n",
        "    cycle_dates.append(Data_05_17_19_strength_EU.iloc[start_index]['날짜'])  # replace 'Date' with your date column name\n",
        "\n",
        "# Standardize the cycles using tslearn's TimeSeriesScalerMeanVariance\n",
        "scaler = TimeSeriesScalerMeanVariance(mu=0., std=1.)  # Rescale time series\n",
        "scaled_cycles = [scaler.fit_transform(cycle.reshape(-1, 1)) for cycle in cycles]\n",
        "\n",
        "# Calculate DTW distance between all pairs of cycles\n",
        "n_cycles = len(scaled_cycles)\n",
        "dist_matrix = np.empty((n_cycles, n_cycles))\n",
        "for i in range(n_cycles):\n",
        "    for j in range(n_cycles):\n",
        "        # Ensure the 2D cycle data is reduced to 1D before running DTW\n",
        "        cycle_i = np.mean(scaled_cycles[i], axis=1)\n",
        "        cycle_j = np.mean(scaled_cycles[j], axis=1)\n",
        "        distance, path = fastdtw(cycle_i, cycle_j, dist=euclidean)\n",
        "        dist_matrix[i, j] = distance\n",
        "\n",
        "# Detect outliers based on DTW distance\n",
        "# A simple method would be to consider cycles with a mean DTW distance larger than the average as outliers\n",
        "mean_dtw_distances = dist_matrix.mean(axis=1)\n",
        "threshold = mean_dtw_distances.mean()\n",
        "outlier_indices = np.where(mean_dtw_distances > threshold)[0]\n",
        "\n",
        "print(f\"Outlier cycles at indices: {outlier_indices}\")\n",
        "for index in outlier_indices:\n",
        "    print(f\"Outlier detected on date: {cycle_dates[index]}\")\n"
      ],
      "metadata": {
        "id": "R8ZRtjBcweYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SR 변환 적용"
      ],
      "metadata": {
        "id": "YyZGq0jdwmLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Current_Strength_US = Current_Strength_US_EU_data[Current_Strength_US_EU_data['Category'] == 'US']['VALUE']\n",
        "Current_Strength_EU = Current_Strength_US_EU_data[Current_Strength_US_EU_data['Category'] == 'EU']['VALUE']"
      ],
      "metadata": {
        "id": "u7GCqhqPwoI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def series_filter(values, kernel_size=3):\n",
        "\n",
        "    filter_values = np.cumsum(values, dtype=float)\n",
        "\n",
        "    filter_values[kernel_size:] = filter_values[kernel_size:] - filter_values[:-kernel_size]\n",
        "    filter_values[kernel_size:] = filter_values[kernel_size:] / kernel_size\n",
        "\n",
        "    for i in range(1, kernel_size):\n",
        "        filter_values[i] /= i + 1\n",
        "\n",
        "    return filter_values\n",
        "\n",
        "\n",
        "def extrapolate_next(values):\n",
        "\n",
        "\n",
        "    last_value = values[-1]\n",
        "    slope = [(last_value - v) / i for (i, v) in enumerate(values[::-1])]\n",
        "    slope[0] = 0\n",
        "    next_values = last_value + np.cumsum(slope)\n",
        "\n",
        "    return next_values\n",
        "\n",
        "\n",
        "def marge_series(values, extend_num=5, forward=5):\n",
        "\n",
        "    next_value = extrapolate_next(values)[forward]\n",
        "    extension = [next_value] * extend_num\n",
        "\n",
        "    if isinstance(values, list):\n",
        "        marge_values = values + extension\n",
        "    else:\n",
        "        marge_values = np.append(values, extension)\n",
        "    return marge_values\n",
        "\n",
        "\n",
        "\n",
        "class Silency(object):\n",
        "    def __init__(self, amp_window_size, series_window_size, score_window_size):\n",
        "        self.amp_window_size = amp_window_size\n",
        "        self.series_window_size = series_window_size\n",
        "        self.score_window_size = score_window_size\n",
        "\n",
        "    def transform_silency_map(self, values):\n",
        "\n",
        "\n",
        "        freq = np.fft.fft(values)\n",
        "        mag = np.sqrt(freq.real ** 2 + freq.imag ** 2)\n",
        "        spectral_residual = np.exp(np.log(mag) - series_filter(np.log(mag), self.amp_window_size))\n",
        "\n",
        "        freq.real = freq.real * spectral_residual / mag\n",
        "        freq.imag = freq.imag * spectral_residual / mag\n",
        "\n",
        "        silency_map = np.fft.ifft(freq)\n",
        "        return silency_map\n",
        "\n",
        "    def transform_spectral_residual(self, values):\n",
        "        silency_map = self.transform_silency_map(values)\n",
        "        spectral_residual = np.sqrt(silency_map.real ** 2 + silency_map.imag ** 2)\n",
        "        return spectral_residual\n",
        "\n",
        "    def generate_anomaly_score(self, values, type=\"avg\"):\n",
        "\n",
        "\n",
        "        extended_series = marge_series(values, self.series_window_size, self.series_window_size)\n",
        "        mag = self.transform_spectral_residual(extended_series)[: len(values)]\n",
        "\n",
        "        if type == \"avg\":\n",
        "            ave_filter = series_filter(mag, self.score_window_size)\n",
        "            score = (mag - ave_filter) / ave_filter\n",
        "        elif type == \"abs\":\n",
        "            ave_filter = series_filter(mag, self.score_window_size)\n",
        "            score = np.abs(mag - ave_filter) / ave_filter\n",
        "        elif type == \"chisq\":\n",
        "            score = stats.chi2.cdf((mag - np.mean(mag)) ** 2 / np.var(mag), df=1)\n",
        "        else:\n",
        "            raise ValueError(\"No type!\")\n",
        "        return score"
      ],
      "metadata": {
        "id": "-v_QlhZHxXBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sr_time_series(time_series, amp_window_size=30, series_window_size=30, score_window_size=3):\n",
        "    # Initialize the Silency class with given window sizes\n",
        "    silency_transformer = Silency(amp_window_size, series_window_size, score_window_size)\n",
        "\n",
        "    #amp_window_size: 스펙트럼 구성 요소의 진폭을 평활화하거나 필터링하는 데 사용되는 창 크기. 이 값을 증가시키면 진폭의 평활화가 더욱 강조되어 스펙트럼에서의 급격한 변동을 줄일 수 있습니다.\n",
        "    #series_window_size: 시계열 데이터를 직접 작업할 때 사용되는 창 크기(예: 시계열 병합 또는 세분화)를 제어할 수 있습니다. 정확한 목적은 시계열에 적용되는 특정 방법이나 알고리즘에 따라 달라집니다.\n",
        "    #SCORE_WINDOW_SIZE: 이는 스펙트럼 잔차의 최종 점수 또는 순위와 관련이 있을 수 있으며, 잔차가 최종 이상치 점수로 변환되는 방식을 제어합니다. 다시 말하지만, 이동 평균 또는 기타 평활화 기법에서 최종 측정값을 도출하는 데 사용될 수 있습니다.\n",
        "\n",
        "    # Transform the time series to spectral residuals\n",
        "    spectral_residuals = silency_transformer.transform_spectral_residual(time_series)\n",
        "\n",
        "    # Return the spectral residuals\n",
        "    return spectral_residuals\n"
      ],
      "metadata": {
        "id": "hin5saOWxd8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming Current_Strength_EU is available, split it into training and testing data\n",
        "length = len(Current_Strength_EU)\n",
        "split_point = int(length * 0.7)\n",
        "\n",
        "EU_data = Current_Strength_EU[:split_point]\n",
        "real = Current_Strength_EU[split_point:]"
      ],
      "metadata": {
        "id": "DISjhlNhxix-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FFT Transformation\n",
        "fft_EU = np.fft.fft(EU_data)\n",
        "fft_real = np.fft.fft(real)\n",
        "\n",
        "fft_EU_magnitude = [abs(value) for value in fft_EU]\n",
        "fft_EU_phase = [np.angle(value) for value in fft_EU]"
      ],
      "metadata": {
        "id": "pNYLIWGKxj2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversion to SR format\n",
        "SR_EU = sr_time_series(EU_data)\n",
        "SR_real = sr_time_series(real)"
      ],
      "metadata": {
        "id": "U6qdGq36xk4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig = make_subplots(rows=3, cols=1, shared_xaxes=True, subplot_titles=(\"EU_data\", \"SR_EU\", \"fft_EU Magnitude\"))\n",
        "\n",
        "# Add EU_data to the first subplot\n",
        "fig.add_trace(\n",
        "    go.Scatter(y=EU_data, mode='lines', name='EU_data'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Add SR_EU data to the second subplot\n",
        "fig.add_trace(\n",
        "    go.Scatter(y=SR_EU, mode='lines', name='SR_EU'),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Add fft_EU Magnitude (in log scale) to the third subplot\n",
        "fig.add_trace(\n",
        "    go.Scatter(y=np.log(np.abs(fft_EU) + 1e-10), mode='lines', name='fft_EU Magnitude (Log Scale)'),\n",
        "    row=3, col=1\n",
        ")\n",
        "\n",
        "# Update xaxis and yaxis properties\n",
        "fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"EU_data Value\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Time\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"SR_EU Value\", row=2, col=1)\n",
        "fig.update_xaxes(title_text=\"Time\", row=3, col=1)\n",
        "fig.update_yaxes(title_text=\"fft_EU Magnitude (Log Scale)\", row=3, col=1)\n",
        "\n",
        "# Update title and show figure\n",
        "fig.update_layout(title_text=\"EU_data, SR_EU, and fft_EU Magnitude Time Series\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "0zdDKo5uxmgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DIF Anomaly Detection 적용"
      ],
      "metadata": {
        "id": "A_s-KmIdxqLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anomaly Detection Model\n",
        "from algorithms.dif import DIF"
      ],
      "metadata": {
        "id": "70C8M6kLxwO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SR_EU = SR_EU.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "xg7W70apyZtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_configs = {\n",
        "    'n_ensemble': 100,\n",
        "    'n_estimators': 8,\n",
        "    'max_samples': 'auto',\n",
        "    'batch_size': 1024,\n",
        "    'n_processes': 4\n",
        "}\n",
        "model_dif = DIF(**model_configs)\n",
        "model_dif.fit(SR_EU)"
      ],
      "metadata": {
        "id": "KRPQ9RY1yapi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real = real.values.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "OurTKm4UydIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIF_score = model_dif.decision_function(real)"
      ],
      "metadata": {
        "id": "L8KER-Iryeaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN을 적용한 모델"
      ],
      "metadata": {
        "id": "0cR6fg_eyua0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# Reshape input for CNN\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "\n",
        "# First Convolutional Layer\n",
        "model.add(Conv1D(64, 3, activation='relu', input_shape=(window_size, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Fully Connected Layer\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(prediction_step))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64, verbose=1, validation_split=0.1)"
      ],
      "metadata": {
        "id": "fGN5bue-yxIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 관찰 코드"
      ],
      "metadata": {
        "id": "_MF_4BggzROl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Prepare sequences for LSTM with 1-dimensional data\n",
        "def create_dataset(dataset, look_back=60):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back)]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "\n",
        "def preprocess_and_train(dataset, window_size):\n",
        "    category = dataset['Category'].iloc[0]\n",
        "    data_subset = dataset[dataset['Category'] == category]['VALUE'].values\n",
        "    SR_data = sr_time_series(data_subset)\n",
        "\n",
        "    X, y = create_dataset(SR_data, window_size)\n",
        "    X = X.reshape(X.shape[0], window_size, 1)\n",
        "\n",
        "    lstm_model = create_lstm_model(window_size)\n",
        "    lstm_model.fit(X, y, epochs=5, batch_size=64)\n",
        "\n",
        "    SR_data_dif = SR_data.reshape(-1, 1)\n",
        "    model_configs = {\n",
        "        'n_ensemble': 100,\n",
        "        'n_estimators': 8,\n",
        "        'max_samples': 'auto',\n",
        "        'batch_size': 1024,\n",
        "        'n_processes': 4\n",
        "    }\n",
        "    model_dif = DIF(**model_configs)\n",
        "    model_dif.fit(SR_data_dif)\n",
        "\n",
        "    return lstm_model, model_dif\n",
        "\n",
        "# Define LSTM model\n",
        "def create_lstm_model(look_back):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, input_shape=(look_back, 1)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "\n",
        "def visualize_results(real, all_predictions, all_anomaly_scores, all_errors):\n",
        "    # Visualize original and transformed data, predicted values, anomaly scores, and errors\n",
        "\n",
        "    transformed_real = sr_time_series(np.array(real))\n",
        "\n",
        "    fig = make_subplots(rows=4, cols=1, shared_xaxes=True, vertical_spacing=0.1)\n",
        "\n",
        "    # Replace 'real_data' with 'real' here\n",
        "    fig.add_trace(go.Scatter(y=real, mode='lines+markers', name='Original Data'), row=1, col=1)\n",
        "    fig.add_trace(go.Scatter(y=transformed_real, mode='lines+markers', name='Transformed Data'), row=2, col=1)\n",
        "    fig.add_trace(go.Scatter(x=list(range(window_size, len(all_predictions)+window_size)), y=all_predictions, mode='lines+markers', name='Predictions'), row=2, col=1)\n",
        "    fig.add_trace(go.Scatter(x=list(range(window_size, len(all_errors)+window_size)), y=all_errors, mode='lines+markers', name='Error'), row=3, col=1)\n",
        "    fig.add_trace(go.Scatter(x=list(range(window_size, len(all_anomaly_scores)+window_size)), y=all_anomaly_scores, mode='lines+markers', name='Anomaly Score'), row=4, col=1)\n",
        "\n",
        "\n",
        "    fig.update_layout(height=1000, title_text=\"Original Data, Transformed Data, Predicted Values, Anomaly Score and Error\")\n",
        "    fig.show()\n",
        "\n",
        "def real_time_visualization(real_data, model):\n",
        "    real_data = np.array(real_data)  # Convert series to numpy array\n",
        "    sr_data = sr_time_series(real_data)\n",
        "    all_windows = []\n",
        "    all_predictions = []\n",
        "    all_anomaly_scores = []\n",
        "    all_errors = []\n",
        "    prediction_buffer = []\n",
        "    buffer_size = 20\n",
        "\n",
        "    for i in range(len(real_data) - window_size):\n",
        "        # 1. Get current window and prediction\n",
        "\n",
        "        current_window = sr_data[i:i+window_size].reshape(1, window_size, 1)\n",
        "        prediction = model.predict(current_window)\n",
        "        error = abs(prediction[0, 0] - sr_data[i+window_size])\n",
        "\n",
        "        # Buffer predictions\n",
        "        prediction_buffer.append(prediction[0, 0])\n",
        "\n",
        "        # 2. Append to lists\n",
        "        all_windows.append(current_window)\n",
        "        all_predictions.append(prediction[0, 0])\n",
        "        all_errors.append(error)\n",
        "\n",
        "        # If we have 20 predictions buffered, compute anomaly scores and clear the buffer\n",
        "        if len(prediction_buffer) == buffer_size:\n",
        "            buffered_anomalies = model_dif.decision_function(np.array(prediction_buffer).reshape(-1, 1))\n",
        "            all_anomaly_scores.extend(buffered_anomalies)\n",
        "            prediction_buffer.clear()\n",
        "\n",
        "        # 3. Visualize results in real-time\n",
        "        clear_output(wait=True)  # Clear previous plots\n",
        "        visualize_results(real_data[:i+window_size+1], all_predictions, all_anomaly_scores, all_errors)\n",
        "        time.sleep(0.5)  # Sleep for a duration before the next iteration to make visualization perceptible\n",
        "\n",
        "    # Calculate anomalies for any remaining buffered predictions\n",
        "    if prediction_buffer:\n",
        "        buffered_anomalies = model_dif.decision_function(np.array(prediction_buffer).reshape(-1, 1))\n",
        "        all_anomaly_scores.extend(buffered_anomalies)\n",
        "\n",
        "    return all_windows, all_predictions, all_anomaly_scores, all_errors\n",
        "\n",
        "\n",
        "def run_visualization(real_data, dataset, window_size, buffer_size):\n",
        "    global fig\n",
        "    fig = make_subplots(rows=8, cols=1)\n",
        "    unique_categories = real_data['Category'].unique()\n",
        "\n",
        "    print(f\"Unique Categories Found: {unique_categories}\")  # Diagnostic print\n",
        "\n",
        "    results = {}\n",
        "    for category in unique_categories:\n",
        "        print(f\"Processing data for: {category}\")  # Diagnostic print\n",
        "\n",
        "        data_subset = real_data[real_data['Category'] == category]\n",
        "        processed_dataset = dataset[dataset['Category'] == category]\n",
        "\n",
        "        print(f\"Entries for {category}: {len(data_subset)}\")  # Diagnostic print\n",
        "\n",
        "        lstm_model, model_dif = preprocess_and_train(processed_dataset, window_size)\n",
        "        windows, predictions, anomaly_scores, errors = real_time_visualization(data_subset, lstm_model, model_dif, window_size, buffer_size)\n",
        "\n",
        "        results[category] = {\n",
        "            'windows': windows,\n",
        "            'predictions': predictions,\n",
        "            'anomaly_scores': anomaly_scores,\n",
        "            'errors': errors\n",
        "        }\n",
        "    return results"
      ],
      "metadata": {
        "id": "3kthinTuzThU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform data\n",
        "SR_EU = sr_time_series(EU_data)"
      ],
      "metadata": {
        "id": "gEB1_bTUzXOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences for training\n",
        "window_size = 60\n",
        "look_back = 60\n",
        "X, y = create_dataset(SR_EU, look_back)\n",
        "\n",
        "# Reshape to [samples, timesteps, features]\n",
        "X = np.reshape(X, (X.shape[0], look_back, 1))\n",
        "\n",
        "# Train LSTM model\n",
        "model = create_lstm_model(look_back)\n",
        "model.fit(X, y, epochs=5, batch_size=64)"
      ],
      "metadata": {
        "id": "Au2q7EztzYEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run real-time visualization\n",
        "\n",
        "data = real[:140]\n",
        "windows, predictions, anomaly_scores, errors = real_time_visualization(data, model)"
      ],
      "metadata": {
        "id": "BloHk8wlzZZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize results\n",
        "visualize_results(real[:140], windows, predictions, anomaly_scores, errors)"
      ],
      "metadata": {
        "id": "f0JobGn7zZn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your setup code\n",
        "dataset = Current_Strength_US_EU_data[:1975000]\n",
        "real_time_data = Current_Strength_US_EU_data[1975500:1976000]\n",
        "window_size = 60\n",
        "buffer_size = 500\n",
        "\n",
        "results = run_visualization(real_time_data, dataset, window_size, buffer_size)"
      ],
      "metadata": {
        "id": "fvQW5UM60-dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Peak 분석 코드"
      ],
      "metadata": {
        "id": "iC4BnUVl0Q7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Create a figure using plotly express\n",
        "fig = px.line(avg_08_07, x=\"Date\", y=\"VALUE\", title=\"Wave Visualization\")\n",
        "\n",
        "# Loop through the wave_dict dictionary\n",
        "for key, wave_df in wave_dict.items():\n",
        "    # Get the peak value and date from the wave dataframe\n",
        "    peak_value = wave_df[\"VALUE\"].max()\n",
        "    peak_date = wave_df[\"Date\"][wave_df[\"VALUE\"] == peak_value].iloc[0]\n",
        "    # Add a vertical dashed line at the peak date using plotly graph objects\n",
        "    fig.add_trace(go.Scatter(x=[peak_date, peak_date], y=[0, peak_value], mode=\"lines\", line=dict(dash=\"dash\"), name=key))\n",
        "    # Get the start and end value and date from the wave dataframe\n",
        "    start_value = wave_df[\"VALUE\"].iloc[0]\n",
        "    start_date = wave_df[\"Date\"].iloc[0]\n",
        "    end_value = wave_df[\"VALUE\"].iloc[-1]\n",
        "    end_date = wave_df[\"Date\"].iloc[-1]\n",
        "    # Add a horizontal dashed line at the start and end value using plotly graph objects\n",
        "    fig.add_trace(go.Scatter(x=[start_date, end_date], y=[start_value, start_value], mode=\"lines\", line=dict(dash=\"dash\"), name=key + \"_start\"))\n",
        "    fig.add_trace(go.Scatter(x=[start_date, end_date], y=[end_value, end_value], mode=\"lines\", line=dict(dash=\"dash\"), name=key + \"_end\"))\n",
        "\n",
        "# Show the figure\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "ydRp4TQc1N30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## anomal 데이터 삽입"
      ],
      "metadata": {
        "id": "oyTzxd8R1TOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Assuming wave_dict and wave_dict_sample are defined as in your previous examples...\n",
        "\n",
        "# Extract all the dataframes from wave_dict_sample and wave_dict\n",
        "sample_frames = [wave_dict_sample[key] for key in sorted(wave_dict_sample.keys())]\n",
        "anomaly_frames = [wave_dict[key] for key in sorted(wave_dict.keys())]\n",
        "\n",
        "# Set the anomaly column\n",
        "for frame in sample_frames:\n",
        "    frame['anomaly'] = 0\n",
        "\n",
        "for frame in anomaly_frames:\n",
        "    frame['anomaly'] = 1\n",
        "\n",
        "# Get a list of all possible insertion points for anomaly data in the sample dataframe\n",
        "insertion_points = [i for i in range(1, len(sample_frames))]\n",
        "\n",
        "# Randomly select from these positions for each anomaly dataframe\n",
        "random_insertion_points = random.sample(insertion_points, len(anomaly_frames))\n",
        "\n",
        "# Sort the selected positions to maintain time order\n",
        "random_insertion_points.sort()\n",
        "\n",
        "# Weave together the anomaly data and the sample data at the selected positions\n",
        "combined_frames = []\n",
        "for i in range(len(sample_frames)):\n",
        "    combined_frames.append(sample_frames[i])\n",
        "    if i in random_insertion_points:\n",
        "        combined_frames.append(anomaly_frames.pop(0))\n",
        "\n",
        "# If there's any anomaly data left, add it to the end\n",
        "if anomaly_frames:\n",
        "    combined_frames.extend(anomaly_frames)\n",
        "\n",
        "# Concatenate all the frames to create the result dataframe\n",
        "result_df = pd.concat(combined_frames, ignore_index=True)\n",
        "\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "id": "vcI-U3nc1VvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = result_df[['VALUE', 'anomaly']]"
      ],
      "metadata": {
        "id": "G4dL7Ux91XYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Extract the data from the DataFrame\n",
        "values = test['VALUE'].tolist()\n",
        "anomaly = test['anomaly'].tolist()\n",
        "\n",
        "# Create the main line plot\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=values, mode='lines', name='VALUE'))\n",
        "\n",
        "# Find the starts and ends of the anomaly sections\n",
        "anomaly_starts = [index for index, value in enumerate(anomaly) if value == 1 and (index == 0 or anomaly[index - 1] == 0)]\n",
        "anomaly_ends = [index for index, value in enumerate(anomaly) if value == 1 and (index == len(anomaly) - 1 or anomaly[index + 1] == 0)]\n",
        "\n",
        "# Add shaded regions for anomalies\n",
        "for (start, end) in zip(anomaly_starts, anomaly_ends):\n",
        "    fig.add_shape(\n",
        "        go.layout.Shape(\n",
        "            type=\"rect\",\n",
        "            xref=\"x\",\n",
        "            yref=\"paper\",\n",
        "            x0=start,\n",
        "            y0=0,\n",
        "            x1=end,\n",
        "            y1=1,\n",
        "            fillcolor=\"gray\",\n",
        "            opacity=0.8,\n",
        "            layer=\"below\",\n",
        "            line_width=0\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Provide layout details\n",
        "fig.update_layout(title=\"VALUE with Anomalies highlighted\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "GlRG-p6K1Yzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM AE를 통합 구체적인 수치 설정\n"
      ],
      "metadata": {
        "id": "21NmHIIv1ec-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new = pd.read_csv('C:/Users/Admin/Downloads/Etri/Dishwashers_Anomaly_Detection/test.csv')"
      ],
      "metadata": {
        "id": "tXCFjRUv1i09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = new.copy()"
      ],
      "metadata": {
        "id": "drz3YHL51n7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'anomaly' column to get the values\n",
        "input_x = df['VALUE'].values.reshape(-1, 1)\n",
        "input_y = df['anomaly'].values"
      ],
      "metadata": {
        "id": "xNePFMHC1o-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def temporalize(X, y, timesteps):\n",
        "    output_X = []\n",
        "    output_y = []\n",
        "\n",
        "    for i in range(len(X) - timesteps):\n",
        "        t = X[i:i+timesteps]\n",
        "        output_X.append(t)\n",
        "        output_y.append(y[i + timesteps])\n",
        "\n",
        "    return np.array(output_X), np.array(output_y)"
      ],
      "metadata": {
        "id": "u4K_fLvq1p4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = 30\n",
        "x, y = temporalize(input_x, input_y, timesteps)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "wMXOTOFz1q58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train, valid, and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2)\n",
        "\n",
        "print(len(x_train))\n",
        "print(len(x_valid))\n",
        "print(len(x_test))"
      ],
      "metadata": {
        "id": "9Kzm4ibq1tmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM Autoencoder 학습 시에는 Normal(0) 데이터만으로 학습할 것이기 때문에 데이터로 부터 normal(0)과 abnormal(1) 데이터를 분리한다.\n",
        "\n",
        "\n",
        "# For training the autoencoder, split 0 / 1\n",
        "x_train_y0 = x_train[y_train == 0]\n",
        "x_train_y1 = x_train[y_train == 1]\n",
        "\n",
        "x_valid_y0 = x_valid[y_valid == 0]\n",
        "x_valid_y1 = x_valid[y_valid == 1]"
      ],
      "metadata": {
        "id": "seOk2Skr1vSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch = 128\n",
        "lr = 0.001\n",
        "\n",
        "# Encoder\n",
        "lstm_ae.add(layers.LSTM(32, input_shape=(timesteps, n_features), return_sequences=True))\n",
        "lstm_ae.add(layers.LSTM(16, return_sequences=False))\n",
        "lstm_ae.add(layers.RepeatVector(timesteps))\n",
        "# Decoder\n",
        "lstm_ae.add(layers.LSTM(16, return_sequences=True))\n",
        "lstm_ae.add(layers.LSTM(32, return_sequences=True))\n",
        "lstm_ae.add(layers.TimeDistributed(layers.Dense(n_features)))\n",
        "\n",
        "lstm_ae.summary()"
      ],
      "metadata": {
        "id": "xor7L5PP1y5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile\n",
        "lstm_ae.compile(loss='mse', optimizer=optimizers.Adam(lr))\n",
        "\n",
        "# fit\n",
        "history = lstm_ae.fit(x_train_y0, x_train_y0,\n",
        "                     epochs=epochs, batch_size=batch,\n",
        "                     validation_data=(x_valid_y0, x_valid_y0))"
      ],
      "metadata": {
        "id": "8YYsRRiK10VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='valid loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch'); plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v5cKaIXl11Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(data):\n",
        "    return data.reshape((data.shape[0], -1))"
      ],
      "metadata": {
        "id": "0ZTCJ_6L12Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_x_predictions = lstm_ae.predict(x_valid)\n",
        "mse = np.mean(np.power(flatten(x_valid) - flatten(valid_x_predictions), 2), axis=1)\n",
        "\n",
        "error_df = pd.DataFrame({'Reconstruction_error':mse,\n",
        "                         'True_class':list(y_valid)})\n",
        "precision_rt, recall_rt, threshold_rt = metrics.precision_recall_curve(error_df['True_class'], error_df['Reconstruction_error'])\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(threshold_rt, precision_rt[1:], label='Precision')\n",
        "plt.plot(threshold_rt, recall_rt[1:], label='Recall')\n",
        "plt.xlabel('Threshold'); plt.ylabel('Precision/Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HfaVoVd-15fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recall과 Precision의 값이 교차되는 지점을 최적의 threshold 지점\n",
        "# best position of threshold\n",
        "index_cnt = [cnt for cnt, (p, r) in enumerate(zip(precision_rt, recall_rt)) if p==r][0]\n",
        "print('precision: ',precision_rt[index_cnt],', recall: ',recall_rt[index_cnt])\n",
        "\n",
        "# fixed Threshold\n",
        "threshold_fixed = threshold_rt[index_cnt]\n",
        "print('threshold: ',threshold_fixed)"
      ],
      "metadata": {
        "id": "N1xwo8Ch17MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x_predictions = lstm_ae.predict(x_test)\n",
        "mse = np.mean(np.power(flatten(x_test) - flatten(test_x_predictions), 2), axis=1)\n",
        "\n",
        "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
        "                         'True_class': y_test.tolist()})\n"
      ],
      "metadata": {
        "id": "WTw8Ifv919Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Generate data for plotting\n",
        "groups = error_df.groupby('True_class')\n",
        "normal_data = groups.get_group(0)\n",
        "break_data = groups.get_group(1)\n",
        "\n",
        "# Create the scatter plots\n",
        "trace_normal = go.Scatter(\n",
        "    x=normal_data.index,\n",
        "    y=normal_data.Reconstruction_error,\n",
        "    mode='markers',\n",
        "    name='Normal',\n",
        "    marker=dict(size=3.5, color='blue')\n",
        ")\n",
        "\n",
        "trace_break = go.Scatter(\n",
        "    x=break_data.index,\n",
        "    y=break_data.Reconstruction_error,\n",
        "    mode='markers',\n",
        "    name='Abnormal',\n",
        "    marker=dict(size=3.5, color='orange')\n",
        ")\n",
        "\n",
        "trace_threshold = go.Scatter(\n",
        "    x=[error_df.index.min(), error_df.index.max()],\n",
        "    y=[threshold_fixed, threshold_fixed],\n",
        "    mode='lines',\n",
        "    name='Threshold',\n",
        "    line=dict(color='red')\n",
        ")\n",
        "\n",
        "# Layout\n",
        "layout = go.Layout(\n",
        "    title='Reconstruction error for different classes',\n",
        "    xaxis=dict(title='Data point index'),\n",
        "    yaxis=dict(title='Reconstruction error'),\n",
        "    legend=dict(y=1, x=1)\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=[trace_normal, trace_break, trace_threshold], layout=layout)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "047KphVE1_3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 셋에 대한 재구성 손실을 threshold를 기준으로 0/1로 나누고 이를 confusion matrix로 표현하였다."
      ],
      "metadata": {
        "id": "_i1eIxLr2BZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classification by threshold\n",
        "pred_y = [1 if e > threshold_fixed else 0 for e in error_df['Reconstruction_error'].values]\n",
        "\n",
        "conf_matrix = metrics.confusion_matrix(error_df['True_class'], pred_y)\n",
        "plt.figure(figsize=(7, 7))\n",
        "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt='d')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Class'); plt.ylabel('True Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lRB-ksno2Ckb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "false_pos_rate, true_pos_rate, thresholds = metrics.roc_curve(error_df['True_class'], error_df['Reconstruction_error'])\n",
        "roc_auc = metrics.auc(false_pos_rate, true_pos_rate,)\n",
        "\n",
        "plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)\n",
        "plt.plot([0,1],[0,1], linewidth=5)\n",
        "\n",
        "plt.xlim([-0.01, 1])\n",
        "plt.ylim([0, 1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Receiver operating characteristic curve (ROC)')\n",
        "plt.ylabel('True Positive Rate'); plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qnAmF33Z2Drf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}